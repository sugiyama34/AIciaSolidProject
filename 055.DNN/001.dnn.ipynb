{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 17515198450052401484,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " locality {\n",
       " }\n",
       " incarnation: 11338870430507831593\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 2610520485351974555\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14445006029\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12717161655920084946\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(10):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(y_train[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28))  # shape of input\n",
    "z = Flatten()(x)  # 28x28 -> 784\n",
    "z = Dense(units=128, activation='relu')(z)  # dense + ReLU\n",
    "p = Dense(units=10, activation='softmax')(z)  # dense + softmax\n",
    "\n",
    "model = Model(\n",
    "    inputs=x,\n",
    "    outputs=p,\n",
    ")  # build DNN model\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])  # compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3),\n",
    "    ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'test.h5'), save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 6.0119 - acc: 0.8627 - val_loss: 1.6106 - val_acc: 0.9230\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0599 - acc: 0.9278 - val_loss: 0.9622 - val_acc: 0.9258\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5328 - acc: 0.9415 - val_loss: 0.7926 - val_acc: 0.9348\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3343 - acc: 0.9516 - val_loss: 0.6077 - val_acc: 0.9348\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2412 - acc: 0.9581 - val_loss: 0.5338 - val_acc: 0.9403\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1820 - acc: 0.9649 - val_loss: 0.4968 - val_acc: 0.9415\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1656 - acc: 0.9661 - val_loss: 0.5146 - val_acc: 0.9429\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1516 - acc: 0.9690 - val_loss: 0.4430 - val_acc: 0.9459\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1321 - acc: 0.9712 - val_loss: 0.5305 - val_acc: 0.9497\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1459 - acc: 0.9703 - val_loss: 0.4185 - val_acc: 0.9508\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1362 - acc: 0.9710 - val_loss: 0.4301 - val_acc: 0.9533\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1310 - acc: 0.9724 - val_loss: 0.4144 - val_acc: 0.9529\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1159 - acc: 0.9738 - val_loss: 0.3879 - val_acc: 0.9518\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1159 - acc: 0.9745 - val_loss: 0.3742 - val_acc: 0.9561\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1182 - acc: 0.9746 - val_loss: 0.4216 - val_acc: 0.9520\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1123 - acc: 0.9759 - val_loss: 0.3794 - val_acc: 0.9523\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1045 - acc: 0.9768 - val_loss: 0.3608 - val_acc: 0.9572\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1017 - acc: 0.9775 - val_loss: 0.3452 - val_acc: 0.9548\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0983 - acc: 0.9797 - val_loss: 0.3713 - val_acc: 0.9559\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0964 - acc: 0.9785 - val_loss: 0.3765 - val_acc: 0.9585\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0958 - acc: 0.9795 - val_loss: 0.4101 - val_acc: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c382a30d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9579"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see accuracy\n",
    "\n",
    "accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is 95.66% (in the author's environment.) Pretty good!\n",
    "\n",
    " 95.66% \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class DenseModel:\n",
    "    def __init__(self, layers=1, hid_dim=128):\n",
    "        self.input = Input(shape=(28, 28), name='input')\n",
    "        self.flatten = Flatten(name='flatten')\n",
    "        self.denses = OrderedDict()\n",
    "        for i in range(layers):\n",
    "            name = 'dense_{}'.format(i)\n",
    "            self.denses[name] = Dense(units=hid_dim, activation='relu', name=name)\n",
    "        self.last = Dense(units=10, activation='softmax', name='last')\n",
    "    \n",
    "    \n",
    "    def build(self):\n",
    "        x = self.input\n",
    "        z = self.flatten(x)\n",
    "        for dense in self.denses.values():\n",
    "            z = dense(z)\n",
    "        p = self.last(z)\n",
    "        \n",
    "        model = Model(inputs=x, outputs=p)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== layers: 1 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 2.3582 - acc: 0.1100 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3014 - acc: 0.1140 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "======== layers: 1 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 2.4564 - acc: 0.1134 - val_loss: 2.3028 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3019 - acc: 0.1139 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "======== layers: 1 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 2.9242 - acc: 0.1127 - val_loss: 2.3083 - val_acc: 0.1055\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3027 - acc: 0.1136 - val_loss: 2.3067 - val_acc: 0.1057\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3019 - acc: 0.1138 - val_loss: 2.3061 - val_acc: 0.1058\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3015 - acc: 0.1138 - val_loss: 2.3061 - val_acc: 0.1058\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3012 - acc: 0.1137 - val_loss: 2.3056 - val_acc: 0.1058\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1138 - val_loss: 2.3057 - val_acc: 0.1059\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1138 - val_loss: 2.3055 - val_acc: 0.1058\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1138 - val_loss: 2.3055 - val_acc: 0.1058\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1138 - val_loss: 2.3055 - val_acc: 0.1057\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1136 - val_loss: 2.3056 - val_acc: 0.1058\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3009 - acc: 0.1138 - val_loss: 2.3053 - val_acc: 0.1059\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3008 - acc: 0.1138 - val_loss: 2.3031 - val_acc: 0.1057\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.1865 - acc: 0.1543 - val_loss: 2.1068 - val_acc: 0.1903\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0832 - acc: 0.1945 - val_loss: 2.0770 - val_acc: 0.1987\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0551 - acc: 0.2010 - val_loss: 2.0573 - val_acc: 0.2048\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0391 - acc: 0.2019 - val_loss: 2.0457 - val_acc: 0.2037\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0251 - acc: 0.2061 - val_loss: 2.0416 - val_acc: 0.2055\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0171 - acc: 0.2065 - val_loss: 2.0594 - val_acc: 0.2083\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0090 - acc: 0.2088 - val_loss: 2.0365 - val_acc: 0.2064\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0051 - acc: 0.2089 - val_loss: 2.0416 - val_acc: 0.2082\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0016 - acc: 0.2084 - val_loss: 2.0366 - val_acc: 0.2075\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9982 - acc: 0.2102 - val_loss: 2.0308 - val_acc: 0.2062\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9914 - acc: 0.2110 - val_loss: 2.0247 - val_acc: 0.2073\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9882 - acc: 0.2119 - val_loss: 2.0185 - val_acc: 0.2083\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9838 - acc: 0.2127 - val_loss: 2.0170 - val_acc: 0.2105\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9817 - acc: 0.2114 - val_loss: 2.0175 - val_acc: 0.2108\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9812 - acc: 0.2137 - val_loss: 2.0162 - val_acc: 0.2120\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9786 - acc: 0.2145 - val_loss: 2.0108 - val_acc: 0.2116\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9768 - acc: 0.2126 - val_loss: 2.0061 - val_acc: 0.2098\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9739 - acc: 0.2164 - val_loss: 2.0184 - val_acc: 0.2180\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9707 - acc: 0.2183 - val_loss: 1.9944 - val_acc: 0.2177\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9687 - acc: 0.2189 - val_loss: 1.9929 - val_acc: 0.2122\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9646 - acc: 0.2210 - val_loss: 1.9827 - val_acc: 0.2233\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9594 - acc: 0.2248 - val_loss: 1.9684 - val_acc: 0.2286\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9409 - acc: 0.2355 - val_loss: 1.9315 - val_acc: 0.2492\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9240 - acc: 0.2415 - val_loss: 1.9261 - val_acc: 0.2453\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9127 - acc: 0.2465 - val_loss: 1.9066 - val_acc: 0.2618\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8923 - acc: 0.2587 - val_loss: 1.8836 - val_acc: 0.2535\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8604 - acc: 0.2637 - val_loss: 1.8635 - val_acc: 0.2769\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8291 - acc: 0.2680 - val_loss: 1.8156 - val_acc: 0.2774\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7990 - acc: 0.2721 - val_loss: 1.7844 - val_acc: 0.2664\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7694 - acc: 0.2827 - val_loss: 1.7611 - val_acc: 0.2899\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7455 - acc: 0.2837 - val_loss: 1.7369 - val_acc: 0.2836\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7338 - acc: 0.2880 - val_loss: 1.7329 - val_acc: 0.2951\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7270 - acc: 0.2899 - val_loss: 1.7288 - val_acc: 0.2849\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7231 - acc: 0.2983 - val_loss: 1.7172 - val_acc: 0.2913\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7134 - acc: 0.3016 - val_loss: 1.7140 - val_acc: 0.3062\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7045 - acc: 0.3029 - val_loss: 1.6986 - val_acc: 0.3070\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7011 - acc: 0.3071 - val_loss: 1.7060 - val_acc: 0.3026\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6969 - acc: 0.3078 - val_loss: 1.6947 - val_acc: 0.3231\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6942 - acc: 0.3057 - val_loss: 1.6981 - val_acc: 0.3043\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6932 - acc: 0.3055 - val_loss: 1.7065 - val_acc: 0.2892\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6901 - acc: 0.3068 - val_loss: 1.6887 - val_acc: 0.3033\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6878 - acc: 0.3071 - val_loss: 1.6896 - val_acc: 0.2879\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6865 - acc: 0.3098 - val_loss: 1.6901 - val_acc: 0.2972\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6854 - acc: 0.3082 - val_loss: 1.6906 - val_acc: 0.3055\n",
      "======== layers: 1 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 4.1164 - acc: 0.1358 - val_loss: 2.2011 - val_acc: 0.1718\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.1352 - acc: 0.1930 - val_loss: 2.0702 - val_acc: 0.2342\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0023 - acc: 0.2461 - val_loss: 1.9601 - val_acc: 0.2646\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8977 - acc: 0.2803 - val_loss: 1.8684 - val_acc: 0.3013\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7572 - acc: 0.3362 - val_loss: 1.7195 - val_acc: 0.3686\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6453 - acc: 0.3658 - val_loss: 1.6197 - val_acc: 0.3848\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5685 - acc: 0.3802 - val_loss: 1.5730 - val_acc: 0.3854\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5222 - acc: 0.3960 - val_loss: 1.5260 - val_acc: 0.4107\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4561 - acc: 0.4256 - val_loss: 1.5350 - val_acc: 0.4625\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4081 - acc: 0.4469 - val_loss: 1.4269 - val_acc: 0.4726\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3523 - acc: 0.4729 - val_loss: 1.3760 - val_acc: 0.5015\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2755 - acc: 0.5159 - val_loss: 1.3036 - val_acc: 0.5164\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2207 - acc: 0.5375 - val_loss: 1.2213 - val_acc: 0.5572\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1708 - acc: 0.5621 - val_loss: 1.1957 - val_acc: 0.5742\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1176 - acc: 0.5827 - val_loss: 1.1018 - val_acc: 0.6020\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0516 - acc: 0.6119 - val_loss: 1.0604 - val_acc: 0.6531\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9505 - acc: 0.6752 - val_loss: 0.9311 - val_acc: 0.7000\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8667 - acc: 0.7139 - val_loss: 0.8551 - val_acc: 0.7306\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7795 - acc: 0.7416 - val_loss: 0.7403 - val_acc: 0.7793\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7140 - acc: 0.7643 - val_loss: 0.7164 - val_acc: 0.7799\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6935 - acc: 0.7711 - val_loss: 0.6990 - val_acc: 0.7804\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6688 - acc: 0.7817 - val_loss: 0.6761 - val_acc: 0.7896\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6514 - acc: 0.7868 - val_loss: 0.6372 - val_acc: 0.8044\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6262 - acc: 0.7994 - val_loss: 0.6128 - val_acc: 0.8102\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6213 - acc: 0.8000 - val_loss: 0.6160 - val_acc: 0.8043\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6060 - acc: 0.8048 - val_loss: 0.6478 - val_acc: 0.7907\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6021 - acc: 0.8084 - val_loss: 0.5898 - val_acc: 0.8174\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5930 - acc: 0.8113 - val_loss: 0.5821 - val_acc: 0.8233\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5864 - acc: 0.8137 - val_loss: 0.5842 - val_acc: 0.8159\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.5854 - acc: 0.8135 - val_loss: 0.5922 - val_acc: 0.8171\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.5825 - acc: 0.8141 - val_loss: 0.6065 - val_acc: 0.8075\n",
      "======== layers: 1 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 6.7974 - acc: 0.1717 - val_loss: 2.0714 - val_acc: 0.2488\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.9525 - acc: 0.2905 - val_loss: 1.8214 - val_acc: 0.3465\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.7144 - acc: 0.3853 - val_loss: 1.5374 - val_acc: 0.4701\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.3727 - acc: 0.5264 - val_loss: 1.2340 - val_acc: 0.5497\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.1507 - acc: 0.5675 - val_loss: 1.0760 - val_acc: 0.6418\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.9899 - acc: 0.6620 - val_loss: 0.9596 - val_acc: 0.6861\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.8935 - acc: 0.6964 - val_loss: 0.9046 - val_acc: 0.7187\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8299 - acc: 0.7142 - val_loss: 0.8334 - val_acc: 0.7239\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7715 - acc: 0.7444 - val_loss: 0.7288 - val_acc: 0.7747\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6787 - acc: 0.7912 - val_loss: 0.6448 - val_acc: 0.8237\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6015 - acc: 0.8246 - val_loss: 0.5898 - val_acc: 0.8354\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5446 - acc: 0.8441 - val_loss: 0.5488 - val_acc: 0.8470\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5061 - acc: 0.8538 - val_loss: 0.5076 - val_acc: 0.8463\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4745 - acc: 0.8604 - val_loss: 0.4899 - val_acc: 0.8670\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4552 - acc: 0.8683 - val_loss: 0.4703 - val_acc: 0.8693\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4467 - acc: 0.8707 - val_loss: 0.4818 - val_acc: 0.8615\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4348 - acc: 0.8747 - val_loss: 0.4662 - val_acc: 0.8666\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4250 - acc: 0.8777 - val_loss: 0.4669 - val_acc: 0.8652\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4176 - acc: 0.8774 - val_loss: 0.4555 - val_acc: 0.8720\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4120 - acc: 0.8799 - val_loss: 0.4533 - val_acc: 0.8708\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4049 - acc: 0.8823 - val_loss: 0.4509 - val_acc: 0.8783\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3959 - acc: 0.8851 - val_loss: 0.4330 - val_acc: 0.8808\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3941 - acc: 0.8864 - val_loss: 0.4267 - val_acc: 0.8852\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3913 - acc: 0.8863 - val_loss: 0.4375 - val_acc: 0.8822\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3894 - acc: 0.8887 - val_loss: 0.4483 - val_acc: 0.8725\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3863 - acc: 0.8895 - val_loss: 0.4296 - val_acc: 0.8842\n",
      "======== layers: 1 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 4.7574 - acc: 0.4746 - val_loss: 1.2861 - val_acc: 0.6367\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0892 - acc: 0.7048 - val_loss: 0.9153 - val_acc: 0.7697\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7961 - acc: 0.8002 - val_loss: 0.6983 - val_acc: 0.8386\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6428 - acc: 0.8382 - val_loss: 0.6219 - val_acc: 0.8545\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5502 - acc: 0.8586 - val_loss: 0.5351 - val_acc: 0.8689\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4813 - acc: 0.8766 - val_loss: 0.4846 - val_acc: 0.8826\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4320 - acc: 0.8902 - val_loss: 0.4593 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3858 - acc: 0.9025 - val_loss: 0.4138 - val_acc: 0.9056\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3417 - acc: 0.9108 - val_loss: 0.3686 - val_acc: 0.9141\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3258 - acc: 0.9161 - val_loss: 0.3485 - val_acc: 0.9207\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2968 - acc: 0.9227 - val_loss: 0.3336 - val_acc: 0.9203\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2759 - acc: 0.9292 - val_loss: 0.3079 - val_acc: 0.9264\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2531 - acc: 0.9331 - val_loss: 0.2986 - val_acc: 0.9266\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2369 - acc: 0.9372 - val_loss: 0.2901 - val_acc: 0.9292\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2248 - acc: 0.9403 - val_loss: 0.2755 - val_acc: 0.9329\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2148 - acc: 0.9429 - val_loss: 0.2760 - val_acc: 0.9331\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2112 - acc: 0.9432 - val_loss: 0.2770 - val_acc: 0.9365\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1955 - acc: 0.9466 - val_loss: 0.2751 - val_acc: 0.9331\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1943 - acc: 0.9469 - val_loss: 0.2458 - val_acc: 0.9421\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1865 - acc: 0.9491 - val_loss: 0.2482 - val_acc: 0.9422\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1814 - acc: 0.9503 - val_loss: 0.2396 - val_acc: 0.9423\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1746 - acc: 0.9521 - val_loss: 0.2546 - val_acc: 0.9361\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1760 - acc: 0.9524 - val_loss: 0.2511 - val_acc: 0.9438\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1696 - acc: 0.9531 - val_loss: 0.2367 - val_acc: 0.9448\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1612 - acc: 0.9563 - val_loss: 0.2245 - val_acc: 0.9467\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1673 - acc: 0.9541 - val_loss: 0.2265 - val_acc: 0.9441\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1556 - acc: 0.9564 - val_loss: 0.2266 - val_acc: 0.9463\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1562 - acc: 0.9569 - val_loss: 0.2195 - val_acc: 0.9478\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1534 - acc: 0.9578 - val_loss: 0.2291 - val_acc: 0.9443\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1518 - acc: 0.9579 - val_loss: 0.2416 - val_acc: 0.9426\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1491 - acc: 0.9577 - val_loss: 0.2314 - val_acc: 0.9461\n",
      "======== layers: 1 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 4.7271 - acc: 0.7372 - val_loss: 0.8882 - val_acc: 0.8037\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7505 - acc: 0.8299 - val_loss: 0.6253 - val_acc: 0.8595\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5054 - acc: 0.8728 - val_loss: 0.4966 - val_acc: 0.8834\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3951 - acc: 0.8951 - val_loss: 0.4466 - val_acc: 0.9002\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3366 - acc: 0.9091 - val_loss: 0.4198 - val_acc: 0.9122\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2937 - acc: 0.9178 - val_loss: 0.3946 - val_acc: 0.9230\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2615 - acc: 0.9265 - val_loss: 0.3740 - val_acc: 0.9269\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2310 - acc: 0.9349 - val_loss: 0.3373 - val_acc: 0.9268\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2175 - acc: 0.9383 - val_loss: 0.3360 - val_acc: 0.9302\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2012 - acc: 0.9431 - val_loss: 0.3088 - val_acc: 0.9387\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1794 - acc: 0.9476 - val_loss: 0.3020 - val_acc: 0.9385\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1731 - acc: 0.9506 - val_loss: 0.2927 - val_acc: 0.9391\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1698 - acc: 0.9514 - val_loss: 0.2751 - val_acc: 0.9432\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1531 - acc: 0.9561 - val_loss: 0.2850 - val_acc: 0.9452\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1404 - acc: 0.9589 - val_loss: 0.2602 - val_acc: 0.9441\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1376 - acc: 0.9603 - val_loss: 0.2504 - val_acc: 0.9505\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1389 - acc: 0.9595 - val_loss: 0.2595 - val_acc: 0.9453\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1364 - acc: 0.9607 - val_loss: 0.2488 - val_acc: 0.9490\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1194 - acc: 0.9656 - val_loss: 0.2500 - val_acc: 0.9511\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1190 - acc: 0.9647 - val_loss: 0.2661 - val_acc: 0.9501\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1069 - acc: 0.9678 - val_loss: 0.2492 - val_acc: 0.9517\n",
      "======== layers: 1 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 28us/sample - loss: 5.2251 - acc: 0.8605 - val_loss: 1.4456 - val_acc: 0.9028\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9091 - acc: 0.9134 - val_loss: 0.6993 - val_acc: 0.9131\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4531 - acc: 0.9265 - val_loss: 0.5506 - val_acc: 0.9248\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3032 - acc: 0.9400 - val_loss: 0.4396 - val_acc: 0.9294\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2088 - acc: 0.9521 - val_loss: 0.4061 - val_acc: 0.9411\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1800 - acc: 0.9578 - val_loss: 0.3658 - val_acc: 0.9429\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1597 - acc: 0.9603 - val_loss: 0.3771 - val_acc: 0.9386\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1372 - acc: 0.9649 - val_loss: 0.3474 - val_acc: 0.9450\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1332 - acc: 0.9648 - val_loss: 0.3601 - val_acc: 0.9419\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1453 - acc: 0.9638 - val_loss: 0.3562 - val_acc: 0.9473\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1313 - acc: 0.9657 - val_loss: 0.3172 - val_acc: 0.9499\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1208 - acc: 0.9694 - val_loss: 0.3134 - val_acc: 0.9482\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1167 - acc: 0.9698 - val_loss: 0.2872 - val_acc: 0.9504\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1082 - acc: 0.9712 - val_loss: 0.3176 - val_acc: 0.9491\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1138 - acc: 0.9712 - val_loss: 0.3063 - val_acc: 0.9516\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1013 - acc: 0.9732 - val_loss: 0.2970 - val_acc: 0.9528\n",
      "======== layers: 1 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 5.3522 - acc: 0.8848 - val_loss: 1.3796 - val_acc: 0.9321\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9336 - acc: 0.9428 - val_loss: 0.9633 - val_acc: 0.9417\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4962 - acc: 0.9580 - val_loss: 0.6873 - val_acc: 0.9534\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3186 - acc: 0.9686 - val_loss: 0.7500 - val_acc: 0.9499\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2310 - acc: 0.9743 - val_loss: 0.6521 - val_acc: 0.9518\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2075 - acc: 0.9761 - val_loss: 0.6149 - val_acc: 0.9566\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1970 - acc: 0.9761 - val_loss: 0.6170 - val_acc: 0.9557\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1699 - acc: 0.9790 - val_loss: 0.5334 - val_acc: 0.9597\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1441 - acc: 0.9809 - val_loss: 0.5425 - val_acc: 0.9577\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1591 - acc: 0.9797 - val_loss: 0.5164 - val_acc: 0.9600\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1274 - acc: 0.9822 - val_loss: 0.6101 - val_acc: 0.9578\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1598 - acc: 0.9803 - val_loss: 0.5770 - val_acc: 0.9595\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1266 - acc: 0.9821 - val_loss: 0.5937 - val_acc: 0.9557\n",
      "======== layers: 1 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 4.9459 - acc: 0.9018 - val_loss: 1.5767 - val_acc: 0.9351\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7882 - acc: 0.9530 - val_loss: 0.8250 - val_acc: 0.9501\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3533 - acc: 0.9679 - val_loss: 0.7128 - val_acc: 0.9495\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2458 - acc: 0.9747 - val_loss: 0.6014 - val_acc: 0.9561\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2173 - acc: 0.9771 - val_loss: 0.6914 - val_acc: 0.9537\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1748 - acc: 0.9789 - val_loss: 0.6150 - val_acc: 0.9595\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1948 - acc: 0.9781 - val_loss: 0.5832 - val_acc: 0.9634\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1797 - acc: 0.9796 - val_loss: 0.4892 - val_acc: 0.9668\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1788 - acc: 0.9803 - val_loss: 0.7092 - val_acc: 0.9543\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1883 - acc: 0.9801 - val_loss: 0.5610 - val_acc: 0.9647\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1920 - acc: 0.9806 - val_loss: 0.6927 - val_acc: 0.9588\n",
      "======== layers: 1 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 5.2354 - acc: 0.9077 - val_loss: 1.0703 - val_acc: 0.9475\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6346 - acc: 0.9584 - val_loss: 0.7464 - val_acc: 0.9534\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3057 - acc: 0.9704 - val_loss: 0.7670 - val_acc: 0.9528\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2206 - acc: 0.9762 - val_loss: 0.5863 - val_acc: 0.9625\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2114 - acc: 0.9779 - val_loss: 0.5315 - val_acc: 0.9604\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1933 - acc: 0.9786 - val_loss: 0.6686 - val_acc: 0.9540\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2405 - acc: 0.9770 - val_loss: 0.7378 - val_acc: 0.9560\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2163 - acc: 0.9784 - val_loss: 0.5833 - val_acc: 0.9649\n",
      "======== layers: 2 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.3017 - acc: 0.1126 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.5334 - acc: 0.1135 - val_loss: 2.3052 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3014 - acc: 0.1140 - val_loss: 2.3045 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3045 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3046 - val_acc: 0.1060\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3045 - val_acc: 0.1060\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3045 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.7242 - acc: 0.1095 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3012 - acc: 0.1139 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1139 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1139 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.9841 - acc: 0.1137 - val_loss: 2.3033 - val_acc: 0.1097\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.2654 - acc: 0.1352 - val_loss: 2.2388 - val_acc: 0.1408\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.1805 - acc: 0.1705 - val_loss: 2.1191 - val_acc: 0.1823\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0789 - acc: 0.2026 - val_loss: 2.0369 - val_acc: 0.2078\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9765 - acc: 0.2442 - val_loss: 1.8751 - val_acc: 0.2853\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8000 - acc: 0.3201 - val_loss: 1.7511 - val_acc: 0.3442\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6892 - acc: 0.3501 - val_loss: 1.6416 - val_acc: 0.3663\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5965 - acc: 0.3592 - val_loss: 1.5588 - val_acc: 0.3713\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5171 - acc: 0.3992 - val_loss: 1.4764 - val_acc: 0.4305\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.4305 - acc: 0.4425 - val_loss: 1.4134 - val_acc: 0.4337\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.3674 - acc: 0.4666 - val_loss: 1.3356 - val_acc: 0.4767\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3010 - acc: 0.5023 - val_loss: 1.2917 - val_acc: 0.5085\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2384 - acc: 0.5392 - val_loss: 1.1950 - val_acc: 0.5633\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1640 - acc: 0.5780 - val_loss: 1.1224 - val_acc: 0.5877\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1196 - acc: 0.5957 - val_loss: 1.1005 - val_acc: 0.5918\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0948 - acc: 0.6073 - val_loss: 1.0655 - val_acc: 0.6144\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0821 - acc: 0.6166 - val_loss: 1.0616 - val_acc: 0.6197\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0727 - acc: 0.6181 - val_loss: 1.1020 - val_acc: 0.6033\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0646 - acc: 0.6207 - val_loss: 1.0400 - val_acc: 0.6274\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0611 - acc: 0.6231 - val_loss: 1.0451 - val_acc: 0.6275\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0518 - acc: 0.6273 - val_loss: 1.0342 - val_acc: 0.6210\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0587 - acc: 0.6195 - val_loss: 1.0614 - val_acc: 0.6066\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0619 - acc: 0.6187 - val_loss: 1.0646 - val_acc: 0.6097\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0485 - acc: 0.6256 - val_loss: 1.0239 - val_acc: 0.6289\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0492 - acc: 0.6276 - val_loss: 1.0370 - val_acc: 0.6228\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0491 - acc: 0.6257 - val_loss: 1.0216 - val_acc: 0.6308\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0523 - acc: 0.6231 - val_loss: 1.0277 - val_acc: 0.6228\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0364 - acc: 0.6281 - val_loss: 1.0609 - val_acc: 0.6027\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0371 - acc: 0.6288 - val_loss: 1.0383 - val_acc: 0.6204\n",
      "======== layers: 2 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.6019 - acc: 0.1812 - val_loss: 1.8733 - val_acc: 0.3326\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6716 - acc: 0.4150 - val_loss: 1.4893 - val_acc: 0.4554\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3826 - acc: 0.4898 - val_loss: 1.2290 - val_acc: 0.5082\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1039 - acc: 0.6049 - val_loss: 1.0064 - val_acc: 0.6390\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9204 - acc: 0.6672 - val_loss: 0.8773 - val_acc: 0.6972\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7731 - acc: 0.7378 - val_loss: 0.7496 - val_acc: 0.7746\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6699 - acc: 0.7805 - val_loss: 0.6611 - val_acc: 0.7760\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6054 - acc: 0.8060 - val_loss: 0.6279 - val_acc: 0.8134\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5628 - acc: 0.8242 - val_loss: 0.5593 - val_acc: 0.8383\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5298 - acc: 0.8394 - val_loss: 0.5367 - val_acc: 0.8561\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5098 - acc: 0.8473 - val_loss: 0.5071 - val_acc: 0.8653\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4889 - acc: 0.8575 - val_loss: 0.4931 - val_acc: 0.8594\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4646 - acc: 0.8643 - val_loss: 0.4774 - val_acc: 0.8572\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4504 - acc: 0.8687 - val_loss: 0.4467 - val_acc: 0.8727\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4401 - acc: 0.8734 - val_loss: 0.4559 - val_acc: 0.8736\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4313 - acc: 0.8767 - val_loss: 0.4529 - val_acc: 0.8795\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4148 - acc: 0.8809 - val_loss: 0.4324 - val_acc: 0.8836\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4041 - acc: 0.8860 - val_loss: 0.4425 - val_acc: 0.8842\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3969 - acc: 0.8857 - val_loss: 0.4164 - val_acc: 0.8886\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3878 - acc: 0.8871 - val_loss: 0.4325 - val_acc: 0.8815\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3742 - acc: 0.8920 - val_loss: 0.4092 - val_acc: 0.8878\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3675 - acc: 0.8939 - val_loss: 0.3980 - val_acc: 0.8945\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3623 - acc: 0.8951 - val_loss: 0.4099 - val_acc: 0.8901\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3586 - acc: 0.8969 - val_loss: 0.4164 - val_acc: 0.8894\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3506 - acc: 0.8996 - val_loss: 0.3892 - val_acc: 0.8957\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3450 - acc: 0.9004 - val_loss: 0.4094 - val_acc: 0.8870\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3422 - acc: 0.9012 - val_loss: 0.3817 - val_acc: 0.8963\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3347 - acc: 0.9028 - val_loss: 0.3730 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3254 - acc: 0.9069 - val_loss: 0.3925 - val_acc: 0.8988\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3228 - acc: 0.9076 - val_loss: 0.3745 - val_acc: 0.9058\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3146 - acc: 0.9107 - val_loss: 0.3549 - val_acc: 0.9064\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3058 - acc: 0.9139 - val_loss: 0.3606 - val_acc: 0.9028\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3031 - acc: 0.9135 - val_loss: 0.3591 - val_acc: 0.9053\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3054 - acc: 0.9129 - val_loss: 0.3591 - val_acc: 0.9063\n",
      "======== layers: 2 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.8024 - acc: 0.5841 - val_loss: 0.9992 - val_acc: 0.7253\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8269 - acc: 0.7670 - val_loss: 0.6731 - val_acc: 0.8248\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5855 - acc: 0.8378 - val_loss: 0.5203 - val_acc: 0.8627\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4645 - acc: 0.8729 - val_loss: 0.4668 - val_acc: 0.8724\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3906 - acc: 0.8922 - val_loss: 0.3895 - val_acc: 0.8935\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3455 - acc: 0.9021 - val_loss: 0.3668 - val_acc: 0.9046\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3184 - acc: 0.9105 - val_loss: 0.3362 - val_acc: 0.9128\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2916 - acc: 0.9172 - val_loss: 0.3094 - val_acc: 0.9163\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2637 - acc: 0.9244 - val_loss: 0.3064 - val_acc: 0.9185\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2494 - acc: 0.9272 - val_loss: 0.2994 - val_acc: 0.9234\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2362 - acc: 0.9320 - val_loss: 0.2822 - val_acc: 0.9267\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2260 - acc: 0.9333 - val_loss: 0.2776 - val_acc: 0.9272\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2141 - acc: 0.9375 - val_loss: 0.2864 - val_acc: 0.9260\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2026 - acc: 0.9409 - val_loss: 0.2619 - val_acc: 0.9311\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2064 - acc: 0.9409 - val_loss: 0.2566 - val_acc: 0.9333\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1983 - acc: 0.9428 - val_loss: 0.2604 - val_acc: 0.9365\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1861 - acc: 0.9464 - val_loss: 0.2502 - val_acc: 0.9347\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1837 - acc: 0.9466 - val_loss: 0.2381 - val_acc: 0.9388\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1744 - acc: 0.9486 - val_loss: 0.2470 - val_acc: 0.9389\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1746 - acc: 0.9481 - val_loss: 0.2425 - val_acc: 0.9384\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1688 - acc: 0.9495 - val_loss: 0.2584 - val_acc: 0.9371\n",
      "======== layers: 2 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.9330 - acc: 0.8102 - val_loss: 1.0707 - val_acc: 0.8892\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7405 - acc: 0.9046 - val_loss: 0.6900 - val_acc: 0.9093\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4503 - acc: 0.9272 - val_loss: 0.4837 - val_acc: 0.9264\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3254 - acc: 0.9401 - val_loss: 0.4426 - val_acc: 0.9299\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2527 - acc: 0.9482 - val_loss: 0.4098 - val_acc: 0.9320\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2058 - acc: 0.9536 - val_loss: 0.3460 - val_acc: 0.9367\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1836 - acc: 0.9579 - val_loss: 0.3643 - val_acc: 0.9381\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1628 - acc: 0.9612 - val_loss: 0.3187 - val_acc: 0.9477\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1449 - acc: 0.9639 - val_loss: 0.3177 - val_acc: 0.9450\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1458 - acc: 0.9654 - val_loss: 0.4024 - val_acc: 0.9375\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1497 - acc: 0.9655 - val_loss: 0.3221 - val_acc: 0.9492\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1295 - acc: 0.9678 - val_loss: 0.3626 - val_acc: 0.9477\n",
      "======== layers: 2 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 3.3741 - acc: 0.8477 - val_loss: 0.8721 - val_acc: 0.9045\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5835 - acc: 0.9259 - val_loss: 0.5972 - val_acc: 0.9264\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3420 - acc: 0.9465 - val_loss: 0.4457 - val_acc: 0.9388\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2307 - acc: 0.9579 - val_loss: 0.4401 - val_acc: 0.9397\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1979 - acc: 0.9627 - val_loss: 0.3638 - val_acc: 0.9484\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1856 - acc: 0.9639 - val_loss: 0.3831 - val_acc: 0.9512\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1491 - acc: 0.9704 - val_loss: 0.3290 - val_acc: 0.9532\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1458 - acc: 0.9702 - val_loss: 0.3551 - val_acc: 0.9549\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1309 - acc: 0.9732 - val_loss: 0.4134 - val_acc: 0.9477\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1362 - acc: 0.9743 - val_loss: 0.3905 - val_acc: 0.9510\n",
      "======== layers: 2 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.3822 - acc: 0.8821 - val_loss: 0.9479 - val_acc: 0.9183\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5525 - acc: 0.9423 - val_loss: 0.6044 - val_acc: 0.9380\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2941 - acc: 0.9598 - val_loss: 0.5440 - val_acc: 0.9449\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2412 - acc: 0.9661 - val_loss: 0.4453 - val_acc: 0.9555\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1998 - acc: 0.9693 - val_loss: 0.4552 - val_acc: 0.9502\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1781 - acc: 0.9727 - val_loss: 0.4715 - val_acc: 0.9543\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1533 - acc: 0.9758 - val_loss: 0.4388 - val_acc: 0.9553\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1536 - acc: 0.9766 - val_loss: 0.4283 - val_acc: 0.9572\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1362 - acc: 0.9792 - val_loss: 0.4425 - val_acc: 0.9539\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1255 - acc: 0.9790 - val_loss: 0.3830 - val_acc: 0.9585\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1174 - acc: 0.9797 - val_loss: 0.3569 - val_acc: 0.9618\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1039 - acc: 0.9815 - val_loss: 0.3506 - val_acc: 0.9618\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0928 - acc: 0.9831 - val_loss: 0.3795 - val_acc: 0.9601\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0817 - acc: 0.9846 - val_loss: 0.3456 - val_acc: 0.9595\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0743 - acc: 0.9848 - val_loss: 0.4005 - val_acc: 0.9549\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0944 - acc: 0.9829 - val_loss: 0.3569 - val_acc: 0.9616\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0849 - acc: 0.9838 - val_loss: 0.3317 - val_acc: 0.9617\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0760 - acc: 0.9845 - val_loss: 0.2783 - val_acc: 0.9670\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0596 - acc: 0.9869 - val_loss: 0.3021 - val_acc: 0.9676\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0553 - acc: 0.9877 - val_loss: 0.2464 - val_acc: 0.9666\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0458 - acc: 0.9893 - val_loss: 0.2558 - val_acc: 0.9671\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0614 - acc: 0.9875 - val_loss: 0.3337 - val_acc: 0.9611\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0660 - acc: 0.9873 - val_loss: 0.2610 - val_acc: 0.9667\n",
      "======== layers: 2 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 2.6367 - acc: 0.8999 - val_loss: 0.5411 - val_acc: 0.9405\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3416 - acc: 0.9524 - val_loss: 0.4943 - val_acc: 0.9402\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1876 - acc: 0.9669 - val_loss: 0.3844 - val_acc: 0.9504\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1338 - acc: 0.9745 - val_loss: 0.3250 - val_acc: 0.9579\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1374 - acc: 0.9747 - val_loss: 0.3361 - val_acc: 0.9579\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1059 - acc: 0.9791 - val_loss: 0.3320 - val_acc: 0.9569\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1010 - acc: 0.9797 - val_loss: 0.3446 - val_acc: 0.9558\n",
      "======== layers: 2 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 30us/sample - loss: 3.6666 - acc: 0.9042 - val_loss: 0.3402 - val_acc: 0.9430\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.1783 - acc: 0.9623 - val_loss: 0.2830 - val_acc: 0.9533\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1156 - acc: 0.9733 - val_loss: 0.2874 - val_acc: 0.9555\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.1173 - acc: 0.9743 - val_loss: 0.2555 - val_acc: 0.9588\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0969 - acc: 0.9782 - val_loss: 0.2508 - val_acc: 0.9567\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.0932 - acc: 0.9796 - val_loss: 0.2434 - val_acc: 0.9624\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0896 - acc: 0.9798 - val_loss: 0.2663 - val_acc: 0.9614\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1128 - acc: 0.9765 - val_loss: 0.2466 - val_acc: 0.9634\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0888 - acc: 0.9798 - val_loss: 0.2564 - val_acc: 0.9631\n",
      "======== layers: 3 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 2.3016 - acc: 0.1132 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "======== layers: 3 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 2.2777 - acc: 0.1814 - val_loss: 2.0910 - val_acc: 0.1953\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.0411 - acc: 0.2118 - val_loss: 2.0113 - val_acc: 0.2186\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.9771 - acc: 0.2275 - val_loss: 1.9600 - val_acc: 0.2297\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.9348 - acc: 0.2357 - val_loss: 1.9219 - val_acc: 0.2389\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8995 - acc: 0.2425 - val_loss: 1.8872 - val_acc: 0.2518\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8682 - acc: 0.2504 - val_loss: 1.8561 - val_acc: 0.2555\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8402 - acc: 0.2386 - val_loss: 1.8282 - val_acc: 0.2466\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8131 - acc: 0.2386 - val_loss: 1.8014 - val_acc: 0.2970\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7896 - acc: 0.2503 - val_loss: 1.7820 - val_acc: 0.3084\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7732 - acc: 0.2679 - val_loss: 1.7659 - val_acc: 0.2686\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7582 - acc: 0.2855 - val_loss: 1.7519 - val_acc: 0.3198\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7439 - acc: 0.3099 - val_loss: 1.7476 - val_acc: 0.3207\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7312 - acc: 0.3278 - val_loss: 1.7245 - val_acc: 0.3429\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7165 - acc: 0.3373 - val_loss: 1.7089 - val_acc: 0.3516\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7014 - acc: 0.3400 - val_loss: 1.6986 - val_acc: 0.3485\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6896 - acc: 0.3411 - val_loss: 1.6819 - val_acc: 0.3539\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6791 - acc: 0.3436 - val_loss: 1.6785 - val_acc: 0.3525\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6692 - acc: 0.3449 - val_loss: 1.6660 - val_acc: 0.3584\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6643 - acc: 0.3434 - val_loss: 1.6638 - val_acc: 0.3677\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6578 - acc: 0.3539 - val_loss: 1.6627 - val_acc: 0.3681\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6540 - acc: 0.3492 - val_loss: 1.6462 - val_acc: 0.3767\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6497 - acc: 0.3585 - val_loss: 1.6505 - val_acc: 0.3271\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6488 - acc: 0.3506 - val_loss: 1.6418 - val_acc: 0.3818\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6445 - acc: 0.3624 - val_loss: 1.6354 - val_acc: 0.3519\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6423 - acc: 0.3618 - val_loss: 1.6327 - val_acc: 0.3904\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6392 - acc: 0.3662 - val_loss: 1.6497 - val_acc: 0.3691\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6382 - acc: 0.3650 - val_loss: 1.6326 - val_acc: 0.3787\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6361 - acc: 0.3706 - val_loss: 1.6330 - val_acc: 0.3798\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6353 - acc: 0.3696 - val_loss: 1.6317 - val_acc: 0.3935\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6346 - acc: 0.3729 - val_loss: 1.6347 - val_acc: 0.3755\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6332 - acc: 0.3732 - val_loss: 1.6298 - val_acc: 0.3862\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6318 - acc: 0.3769 - val_loss: 1.6300 - val_acc: 0.3871\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6326 - acc: 0.3732 - val_loss: 1.6244 - val_acc: 0.3900\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6305 - acc: 0.3771 - val_loss: 1.6248 - val_acc: 0.3893\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6303 - acc: 0.3773 - val_loss: 1.6232 - val_acc: 0.3893\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6288 - acc: 0.3763 - val_loss: 1.6237 - val_acc: 0.3846\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6291 - acc: 0.3787 - val_loss: 1.6207 - val_acc: 0.3878\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6288 - acc: 0.3793 - val_loss: 1.6260 - val_acc: 0.3920\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 1.6282 - acc: 0.3785 - val_loss: 1.6216 - val_acc: 0.3926\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6274 - acc: 0.3793 - val_loss: 1.6232 - val_acc: 0.4002\n",
      "======== layers: 3 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 3.0172 - acc: 0.1127 - val_loss: 2.3033 - val_acc: 0.1056\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.3026 - acc: 0.1139 - val_loss: 2.3026 - val_acc: 0.1058\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.3014 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1059\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1059\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1059\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1059\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1059\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1059\n",
      "======== layers: 3 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 3.3073 - acc: 0.1090 - val_loss: 2.3006 - val_acc: 0.1091\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.2127 - acc: 0.1593 - val_loss: 2.0891 - val_acc: 0.1991\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.0401 - acc: 0.2205 - val_loss: 1.9793 - val_acc: 0.2328\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.9410 - acc: 0.2386 - val_loss: 1.8974 - val_acc: 0.2531\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7850 - acc: 0.2874 - val_loss: 1.6726 - val_acc: 0.3122\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6309 - acc: 0.3464 - val_loss: 1.5748 - val_acc: 0.3729\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.5274 - acc: 0.3972 - val_loss: 1.3914 - val_acc: 0.4252\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.3668 - acc: 0.4578 - val_loss: 1.3335 - val_acc: 0.4760\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.2756 - acc: 0.4980 - val_loss: 1.2272 - val_acc: 0.5293\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.2170 - acc: 0.5142 - val_loss: 1.1645 - val_acc: 0.5362\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1859 - acc: 0.5194 - val_loss: 1.1958 - val_acc: 0.5027\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1637 - acc: 0.5267 - val_loss: 1.1232 - val_acc: 0.5389\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1517 - acc: 0.5280 - val_loss: 1.2157 - val_acc: 0.4688\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1446 - acc: 0.5349 - val_loss: 1.1294 - val_acc: 0.5381\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1346 - acc: 0.5382 - val_loss: 1.1084 - val_acc: 0.5433\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1331 - acc: 0.5402 - val_loss: 1.0982 - val_acc: 0.5610\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.0291 - acc: 0.5976 - val_loss: 0.8918 - val_acc: 0.6458\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.8921 - acc: 0.6578 - val_loss: 0.8645 - val_acc: 0.6729\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.7886 - acc: 0.7336 - val_loss: 0.7048 - val_acc: 0.7823\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6879 - acc: 0.7927 - val_loss: 0.6396 - val_acc: 0.8091\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6454 - acc: 0.8061 - val_loss: 0.6051 - val_acc: 0.8124\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6170 - acc: 0.8150 - val_loss: 0.5993 - val_acc: 0.8207\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6079 - acc: 0.8182 - val_loss: 0.6171 - val_acc: 0.8075\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5866 - acc: 0.8240 - val_loss: 0.5837 - val_acc: 0.8240\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5733 - acc: 0.8300 - val_loss: 0.5796 - val_acc: 0.8301\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5615 - acc: 0.8330 - val_loss: 0.5462 - val_acc: 0.8313\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5558 - acc: 0.8352 - val_loss: 0.5557 - val_acc: 0.8400\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5451 - acc: 0.8366 - val_loss: 0.5371 - val_acc: 0.8465\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5364 - acc: 0.8422 - val_loss: 0.5524 - val_acc: 0.8396\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5229 - acc: 0.8446 - val_loss: 0.5326 - val_acc: 0.8444\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5149 - acc: 0.8474 - val_loss: 0.5279 - val_acc: 0.8487\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5099 - acc: 0.8500 - val_loss: 0.5155 - val_acc: 0.8518\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5004 - acc: 0.8526 - val_loss: 0.5178 - val_acc: 0.8506\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4920 - acc: 0.8560 - val_loss: 0.5139 - val_acc: 0.8500\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4884 - acc: 0.8562 - val_loss: 0.5107 - val_acc: 0.8542\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4816 - acc: 0.8586 - val_loss: 0.4982 - val_acc: 0.8567\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.8580 - val_loss: 0.4957 - val_acc: 0.8516\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4785 - acc: 0.8583 - val_loss: 0.5083 - val_acc: 0.8586\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4720 - acc: 0.8608 - val_loss: 0.5040 - val_acc: 0.8516\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4670 - acc: 0.8632 - val_loss: 0.4833 - val_acc: 0.8583\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4628 - acc: 0.8641 - val_loss: 0.4852 - val_acc: 0.8548\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4632 - acc: 0.8628 - val_loss: 0.4923 - val_acc: 0.8553\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4582 - acc: 0.8656 - val_loss: 0.4980 - val_acc: 0.8538\n",
      "======== layers: 3 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 3.1855 - acc: 0.3107 - val_loss: 1.6562 - val_acc: 0.3772\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.5390 - acc: 0.4282 - val_loss: 1.4318 - val_acc: 0.4787\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.3168 - acc: 0.5174 - val_loss: 1.1574 - val_acc: 0.5738\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.0332 - acc: 0.6382 - val_loss: 0.8908 - val_acc: 0.7077\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.8191 - acc: 0.7301 - val_loss: 0.7004 - val_acc: 0.7883\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6499 - acc: 0.8074 - val_loss: 0.5891 - val_acc: 0.8317\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5723 - acc: 0.8318 - val_loss: 0.5499 - val_acc: 0.8453\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5246 - acc: 0.8475 - val_loss: 0.5261 - val_acc: 0.8533\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4990 - acc: 0.8534 - val_loss: 0.5111 - val_acc: 0.8597\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4807 - acc: 0.8592 - val_loss: 0.4827 - val_acc: 0.8642\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4684 - acc: 0.8630 - val_loss: 0.4717 - val_acc: 0.8618\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4522 - acc: 0.8672 - val_loss: 0.4494 - val_acc: 0.8744\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4402 - acc: 0.8708 - val_loss: 0.4520 - val_acc: 0.8738\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4271 - acc: 0.8739 - val_loss: 0.4392 - val_acc: 0.8742\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4112 - acc: 0.8783 - val_loss: 0.4239 - val_acc: 0.8817\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4069 - acc: 0.8785 - val_loss: 0.4048 - val_acc: 0.8853\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3929 - acc: 0.8841 - val_loss: 0.4138 - val_acc: 0.8827\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3810 - acc: 0.8893 - val_loss: 0.4079 - val_acc: 0.8830\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3705 - acc: 0.8921 - val_loss: 0.3980 - val_acc: 0.8871\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3682 - acc: 0.8922 - val_loss: 0.4171 - val_acc: 0.8817\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3599 - acc: 0.8946 - val_loss: 0.3660 - val_acc: 0.8969\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3394 - acc: 0.9003 - val_loss: 0.3521 - val_acc: 0.8986\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3224 - acc: 0.9049 - val_loss: 0.3514 - val_acc: 0.9013\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3137 - acc: 0.9073 - val_loss: 0.3394 - val_acc: 0.9062\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3032 - acc: 0.9095 - val_loss: 0.3484 - val_acc: 0.9022\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2953 - acc: 0.9120 - val_loss: 0.3340 - val_acc: 0.9087\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2865 - acc: 0.9159 - val_loss: 0.3430 - val_acc: 0.9079\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2798 - acc: 0.9156 - val_loss: 0.3078 - val_acc: 0.9131\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2660 - acc: 0.9201 - val_loss: 0.3064 - val_acc: 0.9170\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2620 - acc: 0.9213 - val_loss: 0.3098 - val_acc: 0.9118\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2623 - acc: 0.9226 - val_loss: 0.3097 - val_acc: 0.9123\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2535 - acc: 0.9255 - val_loss: 0.2930 - val_acc: 0.9181\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2478 - acc: 0.9277 - val_loss: 0.2879 - val_acc: 0.9203\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2432 - acc: 0.9273 - val_loss: 0.2966 - val_acc: 0.9185\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2412 - acc: 0.9277 - val_loss: 0.2924 - val_acc: 0.9153\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2357 - acc: 0.9298 - val_loss: 0.2882 - val_acc: 0.9201\n",
      "======== layers: 3 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 2.9956 - acc: 0.5826 - val_loss: 0.7876 - val_acc: 0.7745\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6352 - acc: 0.8207 - val_loss: 0.4865 - val_acc: 0.8605\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4383 - acc: 0.8757 - val_loss: 0.3908 - val_acc: 0.8911\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3475 - acc: 0.8997 - val_loss: 0.3308 - val_acc: 0.9043\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2977 - acc: 0.9133 - val_loss: 0.3212 - val_acc: 0.9106\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2628 - acc: 0.9217 - val_loss: 0.2816 - val_acc: 0.9205\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2375 - acc: 0.9306 - val_loss: 0.2620 - val_acc: 0.9273\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2199 - acc: 0.9342 - val_loss: 0.2440 - val_acc: 0.9299\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2075 - acc: 0.9378 - val_loss: 0.2699 - val_acc: 0.9259\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1875 - acc: 0.9440 - val_loss: 0.2212 - val_acc: 0.9383\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1770 - acc: 0.9469 - val_loss: 0.2282 - val_acc: 0.9364\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1659 - acc: 0.9501 - val_loss: 0.2170 - val_acc: 0.9417\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1553 - acc: 0.9530 - val_loss: 0.2147 - val_acc: 0.9406\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1519 - acc: 0.9532 - val_loss: 0.2071 - val_acc: 0.9438\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1406 - acc: 0.9573 - val_loss: 0.2169 - val_acc: 0.9429\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1338 - acc: 0.9591 - val_loss: 0.2066 - val_acc: 0.9469\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1266 - acc: 0.9621 - val_loss: 0.2019 - val_acc: 0.9452\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1208 - acc: 0.9633 - val_loss: 0.2019 - val_acc: 0.9457\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1152 - acc: 0.9644 - val_loss: 0.1969 - val_acc: 0.9473\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1102 - acc: 0.9660 - val_loss: 0.1776 - val_acc: 0.9510\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1068 - acc: 0.9670 - val_loss: 0.1867 - val_acc: 0.9498\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1021 - acc: 0.9691 - val_loss: 0.2081 - val_acc: 0.9480\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1009 - acc: 0.9692 - val_loss: 0.1852 - val_acc: 0.9520\n",
      "======== layers: 3 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 2.8904 - acc: 0.7649 - val_loss: 0.5958 - val_acc: 0.8707\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4419 - acc: 0.8959 - val_loss: 0.3750 - val_acc: 0.9089\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2784 - acc: 0.9269 - val_loss: 0.2758 - val_acc: 0.9298\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2142 - acc: 0.9415 - val_loss: 0.2407 - val_acc: 0.9406\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1713 - acc: 0.9508 - val_loss: 0.2519 - val_acc: 0.9373\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1468 - acc: 0.9570 - val_loss: 0.2225 - val_acc: 0.9441\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1276 - acc: 0.9627 - val_loss: 0.2339 - val_acc: 0.9402\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1168 - acc: 0.9648 - val_loss: 0.2018 - val_acc: 0.9513\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1095 - acc: 0.9675 - val_loss: 0.1955 - val_acc: 0.9506\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1007 - acc: 0.9697 - val_loss: 0.1858 - val_acc: 0.9571\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0955 - acc: 0.9725 - val_loss: 0.1814 - val_acc: 0.9572\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0900 - acc: 0.9728 - val_loss: 0.1959 - val_acc: 0.9539\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0922 - acc: 0.9729 - val_loss: 0.1717 - val_acc: 0.9573\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0757 - acc: 0.9774 - val_loss: 0.1850 - val_acc: 0.9573\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0735 - acc: 0.9781 - val_loss: 0.1784 - val_acc: 0.9610\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0741 - acc: 0.9778 - val_loss: 0.2014 - val_acc: 0.9569\n",
      "======== layers: 3 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 2.0171 - acc: 0.8449 - val_loss: 0.5365 - val_acc: 0.9062\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3511 - acc: 0.9252 - val_loss: 0.3689 - val_acc: 0.9216\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2085 - acc: 0.9479 - val_loss: 0.2596 - val_acc: 0.9449\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1497 - acc: 0.9601 - val_loss: 0.2352 - val_acc: 0.9448\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1172 - acc: 0.9659 - val_loss: 0.2288 - val_acc: 0.9488\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1026 - acc: 0.9695 - val_loss: 0.2364 - val_acc: 0.9520\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0892 - acc: 0.9735 - val_loss: 0.2190 - val_acc: 0.9555\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0873 - acc: 0.9752 - val_loss: 0.2134 - val_acc: 0.9568\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0828 - acc: 0.9762 - val_loss: 0.2346 - val_acc: 0.9551\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0775 - acc: 0.9776 - val_loss: 0.2132 - val_acc: 0.9550\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0720 - acc: 0.9794 - val_loss: 0.2336 - val_acc: 0.9521\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0742 - acc: 0.9791 - val_loss: 0.2101 - val_acc: 0.9575\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0645 - acc: 0.9814 - val_loss: 0.2006 - val_acc: 0.9633\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0601 - acc: 0.9823 - val_loss: 0.2006 - val_acc: 0.9593\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0598 - acc: 0.9830 - val_loss: 0.1861 - val_acc: 0.9638\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0593 - acc: 0.9825 - val_loss: 0.1930 - val_acc: 0.9653\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0431 - acc: 0.9876 - val_loss: 0.2007 - val_acc: 0.9613\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0439 - acc: 0.9871 - val_loss: 0.1975 - val_acc: 0.9642\n",
      "======== layers: 3 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 1.6265 - acc: 0.8805 - val_loss: 0.4169 - val_acc: 0.9257\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.2444 - acc: 0.9461 - val_loss: 0.2920 - val_acc: 0.9434\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1524 - acc: 0.9610 - val_loss: 0.2742 - val_acc: 0.9472\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1146 - acc: 0.9705 - val_loss: 0.2204 - val_acc: 0.9578\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0872 - acc: 0.9760 - val_loss: 0.2532 - val_acc: 0.9528\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1042 - acc: 0.9731 - val_loss: 0.2610 - val_acc: 0.9520\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0995 - acc: 0.9740 - val_loss: 0.2393 - val_acc: 0.9577\n",
      "======== layers: 3 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 29us/sample - loss: 1.9019 - acc: 0.8902 - val_loss: 0.3053 - val_acc: 0.9311\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1783 - acc: 0.9553 - val_loss: 0.2067 - val_acc: 0.9498\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1174 - acc: 0.9679 - val_loss: 0.1913 - val_acc: 0.9550\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1054 - acc: 0.9709 - val_loss: 0.2067 - val_acc: 0.9548\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0879 - acc: 0.9759 - val_loss: 0.2221 - val_acc: 0.9542\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0929 - acc: 0.9756 - val_loss: 0.1884 - val_acc: 0.9599\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0802 - acc: 0.9785 - val_loss: 0.1807 - val_acc: 0.9605\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0698 - acc: 0.9810 - val_loss: 0.2027 - val_acc: 0.9584\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0664 - acc: 0.9820 - val_loss: 0.1632 - val_acc: 0.9649\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0679 - acc: 0.9820 - val_loss: 0.1580 - val_acc: 0.9678\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.0615 - acc: 0.9830 - val_loss: 0.1426 - val_acc: 0.9688\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0601 - acc: 0.9842 - val_loss: 0.1584 - val_acc: 0.9672\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0512 - acc: 0.9859 - val_loss: 0.1638 - val_acc: 0.9683\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0468 - acc: 0.9865 - val_loss: 0.1763 - val_acc: 0.9665\n",
      "======== layers: 3 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 2.7564 - acc: 0.9012 - val_loss: 0.1780 - val_acc: 0.9511\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.1173 - acc: 0.9655 - val_loss: 0.1771 - val_acc: 0.9549\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0963 - acc: 0.9717 - val_loss: 0.1883 - val_acc: 0.9574\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.0798 - acc: 0.9766 - val_loss: 0.1694 - val_acc: 0.9578\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.0817 - acc: 0.9767 - val_loss: 0.1451 - val_acc: 0.9666\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.0682 - acc: 0.9807 - val_loss: 0.1981 - val_acc: 0.9607\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0788 - acc: 0.9779 - val_loss: 0.1801 - val_acc: 0.9603\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0600 - acc: 0.9832 - val_loss: 0.1475 - val_acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "dim_hidden_layres = [2**i for i in range(11)]\n",
    "n_layers = range(1, 4)\n",
    "\n",
    "df_accuracy = pd.DataFrame()\n",
    "\n",
    "for layers in n_layers:\n",
    "    for hid_dim in dim_hidden_layres:\n",
    "        print('========', 'layers:', layers, '; hid_dim:', hid_dim, '========')\n",
    "        model = DenseModel(layers=layers, hid_dim=hid_dim)\n",
    "        model = model.build()\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=3),\n",
    "            ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'model_{}_{}.h5'.format(layers, hid_dim)), save_best_only=True),\n",
    "        ]\n",
    "        n_param = model.count_params()\n",
    "        model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)\n",
    "        acc = accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))\n",
    "        \n",
    "        df_accuracy = pd.concat([df_accuracy, pd.DataFrame([[layers, hid_dim, n_param, acc]], columns=['layers', 'hid_dim', 'n_param', 'accuracy'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">n_param</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hid_dim</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>16</th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805</td>\n",
       "      <td>1600</td>\n",
       "      <td>3190</td>\n",
       "      <td>6370</td>\n",
       "      <td>12730</td>\n",
       "      <td>25450</td>\n",
       "      <td>50890</td>\n",
       "      <td>101770</td>\n",
       "      <td>203530</td>\n",
       "      <td>407050</td>\n",
       "      <td>814090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>807</td>\n",
       "      <td>1606</td>\n",
       "      <td>3210</td>\n",
       "      <td>6442</td>\n",
       "      <td>13002</td>\n",
       "      <td>26506</td>\n",
       "      <td>55050</td>\n",
       "      <td>118282</td>\n",
       "      <td>269322</td>\n",
       "      <td>669706</td>\n",
       "      <td>1863690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>809</td>\n",
       "      <td>1612</td>\n",
       "      <td>3230</td>\n",
       "      <td>6514</td>\n",
       "      <td>13274</td>\n",
       "      <td>27562</td>\n",
       "      <td>59210</td>\n",
       "      <td>134794</td>\n",
       "      <td>335114</td>\n",
       "      <td>932362</td>\n",
       "      <td>2913290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_param                                                         \\\n",
       "hid_dim    1     2     4     8      16     32     64      128     256    \n",
       "layers                                                                   \n",
       "1           805  1600  3190  6370  12730  25450  50890  101770  203530   \n",
       "2           807  1606  3210  6442  13002  26506  55050  118282  269322   \n",
       "3           809  1612  3230  6514  13274  27562  59210  134794  335114   \n",
       "\n",
       "                          \n",
       "hid_dim    512      1024  \n",
       "layers                    \n",
       "1        407050   814090  \n",
       "2        669706  1863690  \n",
       "3        932362  2913290  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hid_dim</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>16</th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.3034</td>\n",
       "      <td>0.8084</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0.9617</td>\n",
       "      <td>0.9687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>0.9465</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.3898</td>\n",
       "      <td>0.1134</td>\n",
       "      <td>0.8490</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.9592</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy                                                          \\\n",
       "hid_dim     1       2       4       8       16      32      64      128    \n",
       "layers                                                                     \n",
       "1         0.1135  0.1135  0.3034  0.8084  0.8856  0.9438  0.9503  0.9521   \n",
       "2         0.1135  0.1136  0.1135  0.6219  0.9037  0.9368  0.9442  0.9465   \n",
       "3         0.1135  0.3898  0.1134  0.8490  0.9203  0.9504  0.9592  0.9619   \n",
       "\n",
       "                                 \n",
       "hid_dim    256     512     1024  \n",
       "layers                           \n",
       "1        0.9583  0.9617  0.9687  \n",
       "2        0.9671  0.9577  0.9661  \n",
       "3        0.9583  0.9687  0.9659  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_accuracy.set_index(['layers', 'hid_dim'])[['n_param']].unstack())\n",
    "display(df_accuracy.set_index(['layers', 'hid_dim'])[['accuracy']].unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy.to_csv('dnn_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
